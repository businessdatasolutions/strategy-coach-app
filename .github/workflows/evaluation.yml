name: Strategy Coach Evaluation Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      scenarios:
        description: 'Scenarios to run (all/quick/full)'
        required: false
        default: 'all'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  evaluation:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r tests/evaluation/requirements.txt
    
    - name: Install Playwright browsers
      run: |
        playwright install chromium
        playwright install-deps chromium
    
    - name: Set up environment
      env:
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        echo "ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY" >> .env
        echo "OPENAI_API_KEY=$OPENAI_API_KEY" >> .env
        echo "GOOGLE_API_KEY=$GOOGLE_API_KEY" >> .env
        echo "DEFAULT_LLM_PROVIDER=anthropic" >> .env
        echo "DEFAULT_MODEL=claude-3-5-haiku-20241022" >> .env
        echo "DEBUG=False" >> .env
        echo "LOG_LEVEL=INFO" >> .env
    
    - name: Start API server
      run: |
        nohup uvicorn src.api.main:app --host 0.0.0.0 --port 8000 &
        sleep 10  # Wait for server to start
        curl --retry 5 --retry-delay 2 http://localhost:8000/health
    
    - name: Start Web UI
      run: |
        cd web
        nohup python3 -m http.server 8081 &
        cd ..
        sleep 5  # Wait for web server to start
    
    - name: Run evaluation tests
      id: evaluation
      run: |
        pytest tests/evaluation/test_full_session_evaluation.py \
          -v \
          --html=tests/evaluation/reports/pytest_report.html \
          --self-contained-html \
          --junit-xml=tests/evaluation/reports/junit.xml \
          --maxfail=5 \
          -n auto
      continue-on-error: true
    
    - name: Generate evaluation metrics
      if: always()
      run: |
        python -c "
        import json
        import xml.etree.ElementTree as ET
        from pathlib import Path
        
        # Parse JUnit XML
        tree = ET.parse('tests/evaluation/reports/junit.xml')
        root = tree.getroot()
        
        # Extract metrics
        total = int(root.attrib.get('tests', 0))
        failures = int(root.attrib.get('failures', 0))
        errors = int(root.attrib.get('errors', 0))
        skipped = int(root.attrib.get('skipped', 0))
        
        passed = total - failures - errors - skipped
        success_rate = (passed / total * 100) if total > 0 else 0
        
        metrics = {
            'total_tests': total,
            'passed': passed,
            'failed': failures + errors,
            'skipped': skipped,
            'success_rate': round(success_rate, 2)
        }
        
        # Save metrics
        Path('tests/evaluation/reports/metrics.json').write_text(json.dumps(metrics, indent=2))
        
        # Output for GitHub Actions
        print(f'::notice title=Evaluation Results::Success Rate: {success_rate:.1f}% ({passed}/{total} passed)')
        
        # Set output
        with open('$GITHUB_OUTPUT', 'a') as f:
            f.write(f'success_rate={success_rate}\n')
            f.write(f'passed={passed}\n')
            f.write(f'total={total}\n')
        "
    
    - name: Upload evaluation artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: evaluation-reports-${{ github.run_number }}
        path: |
          tests/evaluation/reports/
          tests/evaluation/screenshots/
        retention-days: 30
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const metrics = JSON.parse(fs.readFileSync('tests/evaluation/reports/metrics.json', 'utf8'));
          
          const comment = `## üéØ Strategy Coach Evaluation Results
          
          | Metric | Value |
          |--------|-------|
          | **Success Rate** | ${metrics.success_rate}% |
          | **Tests Passed** | ${metrics.passed}/${metrics.total_tests} |
          | **Tests Failed** | ${metrics.failed} |
          
          ${metrics.success_rate >= 80 ? '‚úÖ **Quality Gate: PASSED**' : '‚ùå **Quality Gate: FAILED** (min 80% required)'}
          
          [View detailed report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
    
    - name: Quality gate check
      if: always()
      run: |
        SUCCESS_RATE=$(cat tests/evaluation/reports/metrics.json | jq -r '.success_rate')
        if (( $(echo "$SUCCESS_RATE < 80" | bc -l) )); then
          echo "‚ùå Quality gate failed: Success rate ${SUCCESS_RATE}% is below 80% threshold"
          exit 1
        else
          echo "‚úÖ Quality gate passed: Success rate ${SUCCESS_RATE}%"
        fi
    
    - name: Send Slack notification
      if: failure() && github.event_name == 'schedule'
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        text: 'Strategy Coach daily evaluation failed! Success rate below threshold.'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
    
    - name: Update dashboard
      if: always() && github.ref == 'refs/heads/main'
      run: |
        # Update evaluation dashboard (if configured)
        echo "Updating evaluation dashboard with latest metrics..."
        # Add dashboard update logic here

  performance-benchmark:
    runs-on: ubuntu-latest
    needs: evaluation
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Run performance benchmarks
      run: |
        echo "Running performance benchmarks..."
        # Add performance testing logic here
    
    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'customBiggerIsBetter'
        output-file-path: tests/evaluation/reports/benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true